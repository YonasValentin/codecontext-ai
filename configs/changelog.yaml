# CodeContext AI - Changelog Generation Model Training Configuration

model:
  base_model: "codellama/CodeLlama-7b-hf"
  model_name: "codecontext-changelog-7b"
  task_type: "changelog"
  
# Training hyperparameters
training:
  # Core parameters
  num_epochs: 3
  batch_size: 6
  gradient_accumulation_steps: 3
  learning_rate: 2.5e-4
  weight_decay: 0.01
  warmup_steps: 75
  
  # LoRA configuration
  lora_r: 48
  lora_alpha: 96
  lora_dropout: 0.1
  lora_target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  
  # Quantization (QLoRA)
  use_4bit: true
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_quant_type: "nf4"
  use_nested_quant: true
  
  # Optimization
  optimizer: "adamw_torch"
  lr_scheduler_type: "cosine"
  max_grad_norm: 1.0
  
  # Memory management
  gradient_checkpointing: true
  dataloader_pin_memory: false
  remove_unused_columns: false

# Data configuration
data:
  train_file: "data/changelog_train.jsonl"
  eval_file: "data/changelog_eval.jsonl"
  max_seq_length: 1536  # Shorter for changelog entries
  
  # Data preprocessing
  truncation: true
  padding: false
  
  # Special tokens
  instruction_template: |
    ### Instruction:
    Generate a professional changelog entry based on the following commit history and changes:
    
    ### Commit History:
    {commit_history}
    
    ### Changed Files:
    {changed_files}
    
    ### Requirements:
    - Follow semantic versioning principles
    - Group changes by type (Added, Changed, Deprecated, Removed, Fixed, Security)
    - Use clear, user-focused language
    - Include breaking changes prominently
    - Follow Keep a Changelog format
    - Professional tone suitable for release notes
    
    ### Changelog Entry:
  
  response_template: |
    {changelog_entry}

# Evaluation configuration
evaluation:
  eval_steps: 150
  eval_strategy: "steps"
  per_device_eval_batch_size: 3
  evaluation_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  
  # Early stopping
  early_stopping_patience: 3  
  early_stopping_threshold: 0.001

# Logging and monitoring
logging:
  logging_steps: 10
  logging_strategy: "steps"
  report_to: "wandb"
  run_name: "codecontext-changelog-7b"
  
  # Weights & Biases
  wandb_project: "codecontext-ai"
  wandb_entity: null

# Output configuration
output:
  output_dir: "./models/codecontext-changelog-7b"
  save_steps: 300
  save_strategy: "steps"
  save_total_limit: 3
  
  # Model saving
  push_to_hub: false
  hub_model_id: "codecontext/codecontext-changelog-7b"
  hub_private_repo: false

# Hardware optimization
hardware:
  # GPU settings
  bf16: false
  fp16: true
  tf32: true
  
  # CPU settings
  dataloader_num_workers: 4
  
  # Memory optimization
  max_memory_MB: 20000  # Slightly less for changelog model

# Reproducibility
seed: 42
data_seed: 42

# Advanced settings
advanced:
  remove_unused_columns: false
  label_names: ["labels"]
  torch_compile: false
  use_rslora: false
  use_dora: false
  max_steps: -1
  label_smoothing_factor: 0.0