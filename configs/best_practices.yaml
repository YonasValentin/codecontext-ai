# Best Practices Guide Model Training Configuration
model:
  base_model: "Qwen/Qwen3-8B"
  model_name: "codecontext-best-practices-qwen3-8b"
  output_dir: "./models/codecontext-best-practices-qwen3-8b"

# Training parameters optimized for best practices guides
training:
  num_epochs: 3
  learning_rate: 2e-4
  batch_size: 4
  gradient_accumulation_steps: 4
  warmup_steps: 100
  max_steps: 1500
  save_steps: 375
  eval_steps: 100
  logging_steps: 10

# LoRA configuration for efficient fine-tuning
lora:
  r: 16
  alpha: 32
  dropout: 0.1
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  bias: "none"
  task_type: "CAUSAL_LM"

# Dataset configuration for best practices training
data:
  train_file: "datasets/best_practices_train.jsonl"
  validation_file: "datasets/best_practices_val.jsonl"
  max_length: 4096
  test_size: 0.1
  preprocessing_num_workers: 4

# Specialized prompt configuration for best practices guides
prompt:
  instruction_key: "best_practices"
  system_prompt: "You are a senior engineering consultant creating comprehensive best practices guides with production-grade standards and patterns."
  max_input_length: 2048
  max_output_length: 2048

# Training optimization
optimization:
  optim: "adamw_torch"
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  max_grad_norm: 1.0

# Hardware configuration
hardware:
  fp16: true
  dataloader_pin_memory: true
  dataloader_num_workers: 4
  remove_unused_columns: false
  ddp_find_unused_parameters: false

# Evaluation metrics
evaluation:
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  evaluation_strategy: "steps"
  save_strategy: "steps"
  load_best_model_at_end: true

# Weights & Biases integration
wandb:
  project: "codecontext-best-practices"
  name: "best-practices-guide-v1"
  tags: ["best-practices", "codellama", "lora"]